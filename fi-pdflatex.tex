%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% I, the copyright holder of this work, release this work into the
%% public domain. This applies worldwide. In some countries this may
%% not be legally possible; if so: I grant anyone the right to use
%% this work for any purpose, without any conditions, unless such
%% conditions are required by law.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[
  digital, %% The `digital` option enables the default options for the
           %% digital version of a document. Replace with `printed`
           %% to enable the default options for the printed version
           %% of a document.
%%  color,   %% Uncomment these lines (by removing the %% at the
%%           %% beginning) to use color in the digital version of your
%%           %% document
  table,   %% The `table` option causes the coloring of tables.
           %% Replace with `notable` to restore plain LaTeX tables.
  twoside, %% The `twoside` option enables double-sided typesetting.
           %% Use at least 120 g/m² paper to prevent show-through.
           %% Replace with `oneside` to use one-sided typesetting;
           %% use only if you don’t have access to a double-sided
           %% printer, or if one-sided typesetting is a formal
           %% requirement at your faculty.
  lof,     %% The `lof` option prints the List of Figures. Replace
           %% with `nolof` to hide the List of Figures.
  lot,     %% The `lot` option prints the List of Tables. Replace
           %% with `nolot` to hide the List of Tables.
  %% More options are listed in the user guide at
  %% <http://mirrors.ctan.org/macros/latex/contrib/fithesis/guide/mu/fi.pdf>.
]{fithesis3}
%% The following section sets up the locales used in the thesis.
\usepackage[resetfonts]{cmap} %% We need to load the T2A font encoding
\usepackage[T1,T2A]{fontenc}  %% to use the Cyrillic fonts with Russian texts.
\usepackage[
  main=english, %% By using `czech` or `slovak` as the main locale
                %% instead of `english`, you can typeset the thesis
                %% in either Czech or Slovak, respectively.
  english, german, russian, czech, slovak %% The additional keys allow
]{babel}        %% foreign texts to be typeset as follows:
%%
%%   \begin{otherlanguage}{german}  ... \end{otherlanguage}
%%   \begin{otherlanguage}{russian} ... \end{otherlanguage}
%%   \begin{otherlanguage}{czech}   ... \end{otherlanguage}
%%   \begin{otherlanguage}{slovak}  ... \end{otherlanguage}
%%
%% For non-Latin scripts, it may be necessary to load additional
%% fonts:
\usepackage{paratype}
\def\textrussian#1{{\usefont{T2A}{PTSerif-TLF}{m}{rm}#1}}
%%
%% The following section sets up the metadata of the thesis.
\thesissetup{
    date        = \the\year/\the\month/\the\day,
    university  = mu,
    faculty     = fi,
    type        = bc,
    author      = Petr Zelina,
    gender      = m,
    advisor     = {doc. RNDr. Aleš Horák, Ph.D.},
    title       = {Pretraining and Evaluation of Czech ALBERT Language Model},
    TeXtitle    = {Pretraining and Evaluation of Czech ALBERT Language Model},
    keywords    = {NLP, Czech NLP, Neural networks, Deep neural networks, transformers, BERT,  ALBERT, Text classification, Question answering},
    TeXkeywords = {NLP, Czech NLP, Neural networks, Deep neural networks, transformers, BERT,  ALBERT, Text classification, Question answering},
    abstract    = {%
      This is the abstract of my thesis, which can

      span multiple paragraphs.
    },
    thanks      = {%
      Computational resources were supplied by the project "e-Infrastruktura CZ" (e-INFRA LM2018140) provided within the program Projects of Large Research, Development and Innovations Infrastructures.

    },
    bib         = {bib.bib},
    %% Uncomment the following line (by removing the %% at the
    %% beginning) and replace `assignment.pdf` with the filename
    %% of your scanned thesis assignment.
%%    assignment         = assignment.pdf,
}

\usepackage[utf8]{inputenc}

\usepackage{makeidx}      %% The `makeidx` package contains
\makeindex                %% helper commands for index typesetting.
%% These additional packages are used within the document:
\usepackage{paralist} %% Compact list environments
\usepackage{amsmath}  %% Mathematics
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{url}      %% Hyperlinks
\usepackage{markdown} %% Lightweight markup
\usepackage{tabularx} %% Tables
\usepackage{tabu}
\usepackage{booktabs}
\usepackage{listings} %% Source code highlighting
\lstset{
  basicstyle      = \ttfamily,
  identifierstyle = \color{black},
  keywordstyle    = \color{blue},
  keywordstyle    = {[2]\color{cyan}},
  keywordstyle    = {[3]\color{olive}},
  stringstyle     = \color{teal},
  commentstyle    = \itshape\color{magenta},
  breaklines      = true,
}
\usepackage{floatrow} %% Putting captions above tables
\floatsetup[table]{capposition=top}
%% The following code fixes the rendering of BibLaTeX ISO 690
%% references in old TeX Live (such as the one at Overleaf).
\thesisload
\makeatletter
\def\thesis@biblatexiso@fix@package{iso-numeric.bbx}
\def\thesis@biblatexiso@fix@end{\relax}
\newif\ifthesis@biblatexiso@fix@
\thesis@biblatexiso@fix@false
\def\thesis@biblatexiso@fix@next#1,{%
  \def\thesis@biblatexiso@fix@current{#1}%
  \ifx\thesis@biblatexiso@fix@current\thesis@biblatexiso@fix@package
    \thesis@biblatexiso@fix@true
  \fi
  \ifx\thesis@biblatexiso@fix@current\thesis@biblatexiso@fix@end
    \expandafter
    \@gobble
  \fi
  \thesis@biblatexiso@fix@next
}
\expandafter\expandafter\expandafter\thesis@biblatexiso@fix@next\@filelist,\relax,
\ifthesis@biblatexiso@fix@
  \defbibenvironment{bibliography}
    {\list%
       {\MethodFormat}%
       {\setlength{\labelwidth}{\labelnumberwidth}%
        \setlength{\leftmargin}{\labelwidth}%
        \setlength{\labelsep}{\biblabelsep}%
        \addtolength{\leftmargin}{\labelsep}%
        \setlength{\itemsep}{\bibitemsep}%
        \setlength{\parsep}{\bibparsep}}%
        \renewcommand*{\makelabel}[1]{\hss##1}
        }%
    {\endlist}%
  {\item}%
\fi


\hyphenation{ALBERT}


\makeatother
\begin{document}
\chapter*{Introduction}
% \chapter*{Pretraining and Evaluation of Czech ALBERT Language Model}
\addcontentsline{toc}{chapter}{Introduction}


%context
In 2018, researchers from Google released a new language model called BERT~\parencite{bert}. It quickly rose to the top of the leaderboards of many English NLP tasks such as question answering or sentiment analysis. Later, in 2019, they released a leaner version called ALBERT~\parencite{albert}, which is much smaller and achieves similar performance.

%goals
This thesis aims to pretrain an ALBERT model for the Czech language, evaluate its efficiency in several extrinsic tasks, and compare the results to other current state-of-the-art Czech NLP models. 

The first downstream task that is evaluated is the classification of various attitudes and manipulative techniques in text from the Czech Propaganda dataset~\parencite{propaganda}. 

The second task is question answering from the Czech Simple Question Answering Database (SQAD)~\cite{sqad1, sqad2}. Given a paragraph and a question, the goal is to select the sentence from the paragraph, which answers the question the best.

%structure
The first chapter focuses on the most common approaches for applying artificial neural networks to NLP tasks. The second chapter describes the evolution and architecture of language models leading to ALBERT. The third chapter analyzes the development of the Czech ALBERT model. The evaluation process and results are described in the fourth chapter.

%tools
The implementation of the Czech model is based on this GitHub repository~\parencite{albert_repo}, written in TensorFlow v1.15 and Python 3.6. The source code can be found here \parencite{thesis_repo}.

%results
I was able to create a pipeline and to pretrain several Czech ALBERT models. Due to the big processing resorce  requirements of this model, I was not able to achieve similar base performance as the original English models, eve though I was able to utilize MetaCentrum machines \parencite{metacentrum}. Nevertheless the performance on downstream Czech NLP tasks was in some cases comparable to the performance of the state of the art models. I am confident that with more powerful hardware and longer training time, these models could perform even better.

The models I trained can be easily adapted for a broad range o tasks using the Keras library.

% ####################################
% ####################################

\chapter{Machine learning in NLP}
For a long time, natural language processing was dominated by systems with human created rules. These systems are often complex, incomplete, their creation is labor intensive and they are hard to expand. Machine learning offers solutions to some of these problems, but it comes with its own challenges.
\section{Deep learning and NLP}
Even though machine learning approaches to NLP tasks existed, they were often based on shallow models (such as logistic regression or SVM) \parencite{deepNLP}. Part of the reason was the lack of processing power. The other problem is that machine learning expects numbers, but NLP tasks are word based problems. The most common approach was to encode the words as one-hot vectors, meaning the word is encoded to a vector with size of the whole dictionary and all but one component are 0.  These high dimensional sparse vectors cause a lot of problems and the situation is even worse for morphologically rich languages.

We know, that deep learning is successful in many areas, like image recognition or various prediction and classification tasks. From this we can assume that it can be effective in NLP.

In last few years with the increases in processing power, there have been several breakthroughs that made deep learning for NLP feasible and even successful.

\section{Sequence processing}
Language is intrinsically sequential. A traditional way of processing sequences in deep learning is with the use of recurrent neural networks (RNNs). They process the inputs one by one and propagate an internal state across the whole sequence. They are thus able to deal with sequences of variable length. 

A big problem of simple RNNs is the vanishing gradient problem. Because of the architecture of the network and the nature of backpropagation, their memory of is limited. LSTM (Long short-term memory) is a type of an RNN that avoids this problem by using a kind of memory cells, which can be implemented as a neural network component and are trainable.

Another issue is the necessity to process the input sequentialy. This reduces their parallelizability. 

\section{Word embeddings}
Today most models tackle the high dimensionality problem by using word embeddings.  They assign each word a real vector of relatively low dimension (e.g. 300). The embeddings than form vector space, where words with similar meaning are close together. Even better, if some assumptions are satisfied \cite{vectadd} the vectors can be added and subtracted from each other in a meaningful way.  

Word embeddings are usually trained unsupervised in advance on a big corpus. The model tries to predict which words occur in similar context. The most common methods are skip-gram and continuous bag-of-words described by Mikolov et al.~\parencite{mikolov}. The resulting weights than can be used as an initialization for the first layer of the final model.

The main drawback is that these embeddings cannot distinguish polysemy, where the same word can have multiple different meanings. For example the word mouse as an animal and as a pointing device used with a computer. If one meaning is more common than the other, the model usually ignores the other meaning. But when the occurrence is similar, than the vector representation can end up somewhere in between both meanings which is not useful.

\section{Contextualized word embeddings}
To solve the problem with simple word embeddings and polysemy, we need to create a more complex model. A single word by itself is not enough to infer its exact meaning. We need to consider its context while encoding it. 

In many languages the context for some word can be away. It is not sufficient to consider only a small window around the word. It requires more complex architectures with more parameters. These big networks than can have the ability to comprehend the language more than just expressing the meaning of a single word. We can use their representations to infer things about whole sentences or paragraphs. Because they are no longer simple embeddings, we call them language models.


% ####################################
% ####################################

\chapter{Language models}
This chapter shows some of the approaches and architectures for creating more complex language models.

\section{Language models and transfer learning}
For many NLP tasks, it is crucial to understand the underlying system of the language. This system is often complicated. Generally it is hard to gather a large amount of data for the specific task we want the model to learn. Because of this, models trained from scratch are usually not able to comprehend the language and are less accurate.

That is where the idea of transfer learning comes from. We can first pretrain a general language model on big and relatively easy-to-create corpora so that it can learn the inner language structure. Then we can fine-tune the model for the specific task. It has been shown that this approach can increase the quality of the resulting model.

Traditionally language models in NLP were viewed simply from a statistical point of view as a probability distribution over sequences of words/tokens. Well-formed sentences had higher probability than nonsensical ones. They could be used for example for rating possible answers or text generation. The models described below are more powerful. They contain a deeper understanding of the language and we can use their inner language representation as a start-point for almost any NLP task.

 \section{ELMo}
One of the first deep bidirectional word embedding is ELMo~(Embeddings from Language Models)~\parencite{elmo}. It applies a BiLSTM approach. It uses two deep recurrent LSTM networks, one which scans from front to back, and the other which scans from the back to front. The resulting embedding is a linear combination of the hidden states of both LSTMs, after each one scanned the source word (Figure \ref{fig:contextual_models}). The problem this model has is that it cannot deeply consider the context on both sides at once, because both LSTMs see only in one direction and are trained independently. 

\section{Transformers and attention}
Transformer is a neural network architecture whose goal is to transform one sequence to another. It is very effective in machine translation. It was ... by  ... in 2018 \cite{attentionisall}.

Unlike RNNs, transformers ''see'' the whole input sequence at the same time. But this means they have to process a lot of information all at once. When humans try to make sense of a complex sentence, they do not read it all at once, but usually focus on different parts based on the current word/phrase they are reading. Attention is a mechanism, which allows transformers to do something similar.

\subsection{Architecture}

The input of the transformer is a sequence of one-hot encoded words. The first thing is a word embedding to a lower dimension, which is initialized randomly. There is no need to pretrain this embedding, it gets trained with the whole model. 
These embeddings are then fed into a sequence of encoder layers, which creates a hidden representation of the input sequence at its end. The hidden state is then used in a sequence of decoder layers, which create the final output sequence.

\subsection{Encoder layer}
The attention mechanism creates 3 non-contextual representations of each word and uses them to create the output contextual representation. They are called
Let us have a sequence of input words embeddings $w_1, w_2$. When we process $w_1$ 

\subsubsection{Self-attention}


\subsubsection{Multi-head attention}
\subsection{Decoder layer}
Because decoder layers are not used in the BERT architecture, I will not describe them fully. They recursively generate the output sequence using an attention representation of the sequence generated so far as queries into the hidden representation of the encoders.

\subsection{Usage}
This architecture in theory allows for the creation of universal language independent representation of sentences. Each language has its encoder and decoder and we can connect any two of those to create a machine translation model between these two languages. 


\section{GPT}
TODO

\section{BERT}
BERT is a language model developed by Google in 2018~\parencite{bert}. BERT stands for Bidirectional Encoder Representations from Transformers, meaning it uses the encoder part of a transformer for creation of bidirectional contextual word embeddings. 

It is based on the idea of transfer learning. It first has to be pretrained. Google suggests two self-supervised tasks, which can be generated from a corpus with separated sentences. Than its architecture is altered slightly and it can be fine-tuned for the final task.  

At the time of release the model achieved state-of-the-art results in many NLP tasks. 

\subsection{Architecture}
The BERT architecture consists mainly from the encoder part of the transformer. We will use these letters to annotate its parameters (Figure \ref{fig:bert}). The mentioned sizes corresponds to the BERT base model.
\begin{itemize}
\itemsep0em 
\item $V$ -- vocabulary size (e. g. 30 000)
\item $W$ -- maximum length of the input sequence (e. g. 512)
\item $L$ -- number of encoder layers (e. g. 12)
\item $I$ -- intermediate size, width of the encoder layer (e. g. 3072)
\item $H$ -- hidden size, dimensoin of output embedding (e. g. 768)
\item $A$ -- number of attention heads (e. g. 12)
\end{itemize}


\begin{figure}[h]
  \label{fig:bert}
  \begin{center}
    \includegraphics[width=\linewidth]{img/bert.pdf}
  \end{center}
  \vspace{-0.5cm}
  \caption{BERT architectrue and parameters}
\end{figure}

\subsubsection{Model input}

The first input token is always \texttt{[CLS]}. Because of the contextuality of the model, its embedding can contain information from the whole input sequence. It can be than used in classification tasks.

It is often important to know in which part of the sequence a particular token is. To do this, BERT uses position embeddings as a parallel input to the tokens.

Some NLP tasks such as question answering require the model to be able to accept multiple sequences as an input (<Question,Paragraph>). BERT architecture solves this in two ways. The first is to use a special \texttt{[SEP]} token to separate the sequences. The second is segment embedding. It is a parallel input to the tokens and it shows into which sequence the token belongs. This embedding is learned during the training. 

All three embeddings are added together

\subsubsection{}

Unlike ELMo, which has distinct left to right and right to left passes, or GPT, which has only the left context, BERT has truly bidirectional context. (Figure \ref{fig:contextual_models})

\begin{figure}[h]
  \label{fig:contextual_models}
  \begin{center}
    \includegraphics[width=\linewidth]{img/contextual_models.pdf}
  \end{center}
  \vspace{-0.5cm}
  \caption{Comparison of the architecture of contextual models. Source \parencite[page 13]{bert}}
\end{figure}



\subsection{Pretraining}
BERT uses two tasks for pretraining.

\subsubsection{Masked language model (MLM)}
Traditional language models are usually trained by predicting the next word in a sequence. To fully utilize the bidirectional contextuality of BERT, this training objective is altered. The input is the whole sequence, but with a portion of the tokens ''masked'' (usually 15 \%). The goal is to predict the original token. For example \texttt{the cat climbed a~tree~.}~$\xrightarrow{}$~\texttt{the cat climbed a [MASK] .}

This is implemented simply by adding a token classifier on top of the output token embeddings.

Because the \texttt{[MASK]} tokens are only present during pretraining and never for the downstream tasks, the model should get used to not seeing them. The solution proposed in the BERT paper \parencite{bert} is not to always replace the input token with \texttt{[MASK]} token (80 \%), but sometimes use a random token (10 \%) or the correct token (10 \%).  

\subsubsection{Next-sentence prediction (NSP)}
To force the model to create long distance dependencies and to prepare it for multi-input-sequence  tasks, the second pretraining objective is next-sentence prediction. Given a pair of sentences, the goal is to decide whether the second sentence was in the original text directly after the first one.  The model uses the embedding of the \texttt{[CLS]} token in a binary classifier to answer this task.

\subsubsection{}

Both tasks are trained together. An example of their input can look like this:

\texttt{[CLS] the cat climbed a [MASK] . [SEP] than it jumped \newline down .}
\newline
Or a negative example for NSP:

\texttt{[CLS] the cat climbed a [MASK] . [SEP] there are several tests [MASK] primality .}


\subsection{Fine-tuning}

\subsection{Achievements}

\section{ALBERT}
In 2019 Google released a leaner version of BERT called ALBERT~\parencite{albert}, which has much less parameters and achieves better performance. Because it is faster, it can be trained for a longer time on a bigger corpus. They claim it is 18 times smaller and achieves similar performance as BERT in 3/5 of the training time.

\subsection{Differences from BERT}
Lan et al. made three main improvements.
\subsubsection{Factorized embedding parameterization}
\subsubsection{Cross-layer parameter sharing}
\subsubsection{Inter-sentence coherence loss}

\subsection{SentencePiece}

\subsection{Achievements}

% ####################################
% ####################################

\chapter{Czech ALBERT}
\section{Corpus and text preprocessing}
\section{Model parameters}
\section{Pretraining process}
\section{Adapting the model for specific tasks}
\subsection{Keras integration}

% ####################################
% ####################################

\chapter{Evaluation}
\section{Pretraining evaluation}
\section{Other language models suitable for Czech}
\section{Downstream tasks}
\section{Text classification}
\subsection{Propaganda dataset}
\subsection{Model architecture}
\subsection{Evaluation method}
\subsection{Results}
\section{Question answering}
\subsection{Czech SQAD dataset}
\subsection{Model architecture}
\subsection{Evaluation method}
\subsection{Results}
\section{Summary}

% ####################################
% ####################################

\chapter{Conclusion}








datasets
results
Conclusion
chapters
results
further usefulness
further research


% ####################################
% ####################################
% ####################################
% ####################################


% \chapter{These are}
% \section{the available}
% \subsection{sectioning}
% \subsubsection{commands.}
% \paragraph{Paragraphs and}
% \subparagraph{subparagraphs are available as well.}
% Inside the text, you can also use unnumbered lists,
% \begin{itemize}
%   \item such as
%   \item this one
%   \begin{itemize}
%     \item     and they can be nested as well.
%     \item[>>] You can even turn the bullets into something fancier,
%     \item[\S] if you so desire.
%   \end{itemize}
% \end{itemize}
% Numbered lists are
% \begin{enumerate}
%   \item very
%   \begin{enumerate}
%     \item similar
%   \end{enumerate}
% \end{enumerate}
% and so are description lists:
% \begin{description}
%   \item[Description list]
%     A list of terms with a description of each term
% \end{description}
% The spacing of these lists is geared towards paragraphs of text.
% For lists of words and phrases, the \textsf{paralist} package
% offers commands
% \begin{compactitem}
%   \item that
%   \begin{compactitem}
%     \item are
%     \begin{compactitem}
%       \item better
%       \begin{compactitem}
%         \item suited
%       \end{compactitem}
%     \end{compactitem}
%   \end{compactitem}
% \end{compactitem}
% \begin{compactenum}
%   \item to
%   \begin{compactenum}
%     \item this
%     \begin{compactenum}
%       \item kind of
%       \begin{compactenum}
%         \item content.
%       \end{compactenum}
%     \end{compactenum}
%   \end{compactenum}
% \end{compactenum}
% The \textsf{amsthm} package provides the commands necessary for the
% typesetting of mathematical definitions, theorems, lemmas and
% proofs.

% %% We will define several mathematical sectioning commands.
% \newtheorem{theorem}{Theorem}[section] %% The numbering of theorems
%                               %% will be reset after each section.
% \newtheorem{lemma}[theorem]{Lemma}         %% The numbering of lemmas
% \newtheorem{corollary}[theorem]{Corollary} %% and corollaries will
%                               %% share the counter with theorems.
% \theoremstyle{definition}
% \newtheorem{definition}{Definition}
% \theoremstyle{remark}
% \newtheorem*{remark}{Remark}

% \begin{theorem}
%   This is a theorem that offers a profound insight into the
%   mathematical sectioning commands.
% \end{theorem}
% \begin{theorem}[Another theorem]
%   This is another theorem. Unlike the first one, this theorem has
%   been endowed with a name.
% \end{theorem}
% \begin{lemma}
%   Let us suppose that $x^2+y^2=z^2$. Then
%   \begin{equation}
%     \biggl\langle u\biggm|\sum_{i=1}^nF(e_i,v)e_i\biggr\rangle
%     =F\biggl(\sum_{i=1}^n\langle e_i|u\rangle e_i,v\biggr).
%   \end{equation}
% \end{lemma}
% \begin{proof}
%   $\nabla^2 f(x,y)=\frac{\partial^2f}{\partial x^2}+
%   \frac{\partial^2f}{\partial y^2}$.
% \end{proof}
% \begin{corollary}
%   This is a corollary.
% \end{corollary}
% \begin{remark}
%   This is a remark.
% \end{remark}

% \chapter{Floats and references}
% \begin{figure}
%   \begin{center}
%     %% PNG and JPG images can be inserted into the document as well,
%     %% but their resolution needs to be adequate. The minimum is
%     %% about 100 pixels per 1 centimeter or 300 pixels per 1 inch.
%     %% That means that a JPG or PNG image typeset at 4 × 4 cm should
%     %% be 400 × 400 px large at the bare minimum.
%     %%
%     %% The optimum is about 250 pixels per 1 centimeter or 600
%     %% pixels per 1 inch. That means that a JPG or PNG image typeset
%     %% at 4 × 4 cm should be 1000 × 1000 px large or larger.
%     \includegraphics[width=4cm]{fithesis/logo/mu/fithesis-base.pdf}
%   \end{center}
%   \caption{The logo of the Masaryk University at 40\,mm}
%   \label{fig:mulogo1}
% \end{figure}

% \begin{figure}
%   \begin{center}
%     \begin{minipage}{.66\textwidth}
%       \includegraphics[width=\textwidth]{fithesis/logo/mu/fithesis-base.pdf}
%     \end{minipage}
%     \begin{minipage}{.33\textwidth}
%       \includegraphics[width=\textwidth]{fithesis/logo/mu/fithesis-base.pdf} \\
%       \includegraphics[width=\textwidth]{fithesis/logo/mu/fithesis-base.pdf}
%     \end{minipage}
%   \end{center}
%   \caption{The logo of the Masaryk University at $\frac23$ and
%     $\frac13$ of text width}
%   \label{fig:mulogo2}
% \end{figure}

% \begin{table}
%   \begin{tabularx}{\textwidth}{lllX}
%     \toprule
%     Day & Min Temp & Max Temp & Summary \\
%     \midrule
%     Monday & $13^{\circ}\mathrm{C}$ & $21^\circ\mathrm{C}$ & A
%     clear day with low wind and no adverse current advisories. \\
%     Tuesday & $11^{\circ}\mathrm{C}$ & $17^\circ\mathrm{C}$ & A
%     trough of low pressure will come from the northwest. \\
%     Wednesday & $10^{\circ}\mathrm{C}$ &
%     $21^\circ\mathrm{C}$ & Rain will spread to all parts during the
%     morning. \\
%     \bottomrule
%   \end{tabularx}
%   \caption{A weather forecast}
%   \label{tab:weather}
% \end{table}

% The logo of the Masaryk University is shown in Figure
% \ref{fig:mulogo1} and Figure \ref{fig:mulogo2} at pages
% \pageref{fig:mulogo1} and \pageref{fig:mulogo2}. The weather
% forecast is shown in Table \ref{tab:weather} at page
% \pageref{tab:weather}. The following chapter is Chapter
% \ref{chap:matheq} and starts at page \pageref{chap:matheq}.
% Items \ref{item:star1}, \ref{item:star2}, and
% \ref{item:star3} are starred in the following list:
% \begin{compactenum}
%   \item some text
%   \item some other text
%   \item $\star$ \label{item:star1}
%   \begin{compactenum}
%     \item some text
%     \item $\star$ \label{item:star2}
%     \item some other text
%     \begin{compactenum}
%       \item some text
%       \item some other text
%       \item yet another piece of text
%       \item $\star$ \label{item:star3}
%     \end{compactenum}
%     \item yet another piece of text
%   \end{compactenum}
%   \item yet another piece of text
% \end{compactenum}
% If your reference points to a place that has not yet been typeset,
% the \verb"\ref" command will expand to \textbf{??} during the first
% run of
% \texttt{pdflatex \jobname.tex}
% and a second run is going to be needed for the references to
% resolve. With online services -- such as Overleaf -- this is
% performed automatically.

% \chapter{Mathematical equations}
% \label{chap:matheq}
% \TeX{} comes pre-packed with the ability to typeset inline
% equations, such as $\mathrm{e}^{ix}=\cos x+i\sin x$, and display
% equations, such as \[
%   \mathbf{A}^{-1} = \begin{bmatrix}
%   a & b \\ c & d \\
%   \end{bmatrix}^{-1} =
%   \frac{1}{\det(\mathbf{A})} \begin{bmatrix}
%   \,\,\,d & \!\!-b \\ -c & \,a \\
%   \end{bmatrix} =
%   \frac{1}{ad - bc} \begin{bmatrix}
%   \,\,\,d & \!\!-b \\ -c & \,a \\
%   \end{bmatrix}.
% \] \LaTeX{} defines the automatically numbered \texttt{equation}
% environment:
% \begin{equation}
%   \gamma Px = PAx = PAP^{-1}Px.
% \end{equation}
% The package \textsf{amsmath} provides several additional
% environments that can be used to typeset complex equations:
% \begin{enumerate}
%   \item An equation can be spread over multiple lines using the
%     \texttt{multline} environment:
%     \begin{multline}
%       a + b + c + d + e + f + b + c + d + e + f + b + c + d + e +
% f \\
%       + f + g + h + i + j + k + l + m + n + o + p + q
%     \end{multline}

%   \item Several aligned equations can be typeset using the
%     \texttt{align} environment:
%     \begin{align}
%               a + b &= c + d     \\
%                   u &= v + w + x \\[1ex]
%       i + j + k + l &= m
%     \end{align}

%   \item The \texttt{alignat} environment is similar to
%     \texttt{align}, but it doesn't insert horizontal spaces between
%     the individual columns:
%     \begin{alignat}{2}
%       a + b + c &+ d       &   &= 0 \\
%               e &+ f + g   &   &= 5
%     \end{alignat}

%   \item Much like chapter, sections, tables, figures, or list
%     items, equations -- such as \eqref{eq:first} and
%     \eqref{eq:mine} -- can also be labeled and referenced:
%     \begin{alignat}{4}
%       b_{11}x_1 &+ b_{12}x_2  &  &+ b_{13}x_3  &  &             &
%         &= y_1,                   \label{eq:first} \\
%       b_{21}x_1 &+ b_{22}x_2  &  &             &  &+ b_{24}x_4  &
%         &= y_2. \tag{My equation} \label{eq:mine}
%     \end{alignat}

%   \item The \texttt{gather} environment makes it possible to
%     typeset several equations without any alignment:
%     \begin{gather}
%       \psi = \psi\psi, \\
%       \eta = \eta\eta\eta\eta\eta\eta, \\
%       \theta = \theta.
%     \end{gather}

%   \item Several cases can be typeset using the \texttt{cases}
%     environment:
%     \begin{equation}
%       |y| = \begin{cases}
%         \phantom-y & \text{if }z\geq0, \\
%                 -y & \text{otherwise}.
%       \end{cases}
%     \end{equation}
% \end{enumerate}
% For the complete list of environments and commands, consult the
% \textsf{amsmath} package manual\footnote{
%   See \url{http://mirrors.ctan.org/macros/latex/required/amslatex/math/amsldoc.pdf}.
%   The \texttt{\textbackslash url} command is provided by the
%   package \textsf{url}.
% }.

% \chapter{\textnormal{We \textsf{have} \texttt{several} \textsc{fonts}
%   \textit{at} \textbf{disposal}}}
% The serified roman font is used for the main body of the text.
% \textit{Italics are typically used to denote emphasis or
% quotations.} \texttt{The teletype font is typically used for source
% code listings.} The \textbf{bold}, \textsc{small-caps} and
% \textsf{sans-serif} variants of the base roman font can be used to
% denote specific types of information.

% \tiny We \scriptsize can \footnotesize also \small change \normalsize
% the \large font \Large size, \LARGE although \huge it \Huge
% is \huge usually \LARGE not \Large necessary.\normalsize

% A wide variety of mathematical fonts is also available, such as: \[
%   \mathrm{ABC}, \mathcal{ABC}, \mathbf{ABC}, \mathsf{ABC},
%   \mathit{ABC}, \mathtt{ABC}
% \] By loading the \textsf{amsfonts} packages, several additional
% fonts will become available: \[
%   \mathfrak{ABC}, \mathbb{ABC}
% \] Many other mathematical fonts are available\footnote{
%   See \url{http://tex.stackexchange.com/a/58124/70941}.
% }.

% \chapter{Using lightweight markup}
% \shorthandoff{-}
% \begin{markdown*}{%
%   hybrid,
%   definitionLists,
%   footnotes,
%   inlineFootnotes,
%   hashEnumerators,
%   fencedCode,
%   citations,
%   citationNbsps,
% }

% If you decide that \LaTeX{} is too wordy for some parts of your
% document, there are [packages](https://www.ctan.org/pkg/markdown
% "Markdown") that allow you to use more lightweight markup next
% to it.

%  ![logo](fithesis/logo/mu/fithesis-base.pdf "The logo of the
%   Masaryk University")

% This is a bullet list. Unlike numbered lists, bulleted lists
% contain an **unordered** set of bullet points. When a bullet point
% contains multiple paragraphs, the list is typeset as follows:

%   * The first item of a bullet list

%     that spans several paragraphs,
%   * the second item of a bullet list,
%   * the third item of a bullet list.

% When none of the bullet points contains multiple paragraphs, the
% list has a more compact form:

%   * The first item of a bullet list,
%   * the second item of a bullet list,
%   * the third item of a bullet list.

% Unlike a bulleted list, a numbered list implies chronology or
% ordering of the bullet points. When a bullet point
% contains multiple paragraphs, the list is typeset as follows:

%   1. The first item of an ordered list

%      that spans several paragraphs,
%   2. the second item of an ordered list,
%   3. the third item of an ordered list.
%   #. If you are feeling lazy,
%   #. you can use hash enumerators as well.

% When none of the bullet points contains multiple paragraphs, the
% list has a more compact form:

%   6. The first item of an ordered list,
%   7. the second item of an ordered list,
%   8. the third item of an ordered list.

% Definition lists are used to provide definitions of terms. When
% a definition contains multiple paragraphs, the list is typeset
% as follows:

% Term 1

% :   Definition 1

% *Term 2*

% :   Definition 2

%         Some code, part of Definition 2

%     Third paragraph of Definition 2.

% When none of the bullet points contains multiple paragraphs, the
% list has a more compact form:

% Term 1
% :   Definition 1
% *Term 2*
% :   Definition 2

% Block quotations are used to include an excerpt from an external
% document in way that visually clearly separates the excerpt from
% the rest of the work:

% > This is the first level of quoting.
% >
% > > This is nested blockquote.
% >
% > Back to the first level.

% Footnotes are used to include additional information to the
% document that are not necessary for the understanding of the main
% text. Here is a footnote reference^[Here is the footnote.] and
% another.[^longnote]

% [^longnote]: Here's one with multiple blocks.

%     Subsequent paragraphs are indented to show that they
% belong to the previous footnote.

%         Some code

%     The whole paragraph can be indented, or just the first
%     line.  In this way, multi-paragraph footnotes work like
%     multi-paragraph list items.

% Citations are used to provide bibliographical references to other
% documents. This is a regular citation~[@borgman03, p. 123]. This is
% an in-text citation: @borgman03\. You can also cite several authors
% at once using both regular~[see @borgman03, p. 123; @greenberg98,
% sec.  3.2; and @thanh01] and in-text citations: @borgman03 [p.123;
% @greenberg98, sec. 3.2; @thanh01].

% Code blocks are used to include source code listings into the
% document:

%     #include <stdio.h>
%     #include <unistd.h>
%     #include <sys/types.h>
%     #include <sys/wait.h>
%     // This is a comment
%     int main(int argc, char **argv)
%     {
%         while (--c > 1 && !fork());
%         sleep(c = atoi(v[c]));
%         printf("%d\n", c);
%         wait(0);
%         return 0;
%     }

% There is an alternative syntax for code blocks that allows you to
% specify additional information, such as the language of the source
% code. This information can be used for syntax highlighting:

% ``` sh
% #!/bin/sh
% fac() {
%   if [ "$1" -leq 1 ]; then
%     echo 1
%   else
%     echo $(("$1" * fac $(("$1" - 1))))
%   fi
% }
% ``````````````

% ~~~~~~ Ruby
% # Here's a way to empty an array.
% joe = [ 'eggs.', 'some', 'break', 'to', 'Have' ]
% print(joe.pop, " ") while joe.size > 0
% print "\n"
% ~~~~~~

% \end{markdown*}
% \shorthandon{-}

% \chapter{Inserting the bibliography}
% After linking a bibliography data\-base files to the document using
% the \verb"\"\texttt{thesis\discretionary{-}{}{}setup\{bib\discretionary{=}{=}{=}%
% \{\textit{file1},\textit{file2},\,\ldots\,\}\}} command, you can
% start citing the entries. This is just dummy text
% \parencite{borgman03} lightly sprinkled with citations
% \parencite[p.~123]{greenberg98}. Several sources can be cited at
% once: \cite{borgman03,greenberg98,thanh01}.
% \citetitle{greenberg98} was written by \citeauthor{greenberg98} in
% \citeyear{greenberg98}. We can also produce \textcite{greenberg98}%
% \ or %% Let us define a compound command:
% \def\citeauthoryear#1{(\textcite{#1},~\citeyear{#1})}%
% \citeauthoryear{greenberg98}%
% . The full bibliographic citation is:
% \emph{\fullcite{greenberg98}}. We can easily insert a bibliographic
% citation into the footnote\footfullcite{greenberg98}.

% The \verb"\nocite" command will not generate any
% output\nocite{muni}, but it will insert its arguments into
% the bibliography. The \verb"\nocite{*}" command will insert all the
% records in the bibliography database file into the bibliography.
% Try uncommenting the command
% %% \nocite{*}
% and watch the bibliography section come apart at the seams.

% When typesetting the document for the first time, citing a
% \texttt{work} will expand to [\textbf{work}] and the
% \verb"\printbibliography" command will produce no output. It is now
% necessary to generate the bibliography by running \texttt{biber
% \jobname.bcf} from the command line and then by typesetting the
% document again twice. During the first run, the bibliography
% section and the citations will be typeset, and in the second run,
% the bibliography section will appear in the table of contents.

% The \texttt{biber} command needs to be executed from within the
% directory, where the \LaTeX\ source file is located. In Windows,
% the command line can be opened in a directory by holding down the
% \textsf{Shift} key and by clicking the right mouse button while
% hovering the cursor over a directory.  Select the \textsf{Open
% Command Window Here} option in the context menu that opens shortly
% afterwards.

% With online services -- such as Overleaf -- or when using an
% automatic tool -- such as \LaTeX MK -- all commands are executed
% automatically. When you omit the \verb"\printbibliography" command,
% its location will be decided by the template.

% \printbibliography[heading=bibintoc] %% Print the bibliography.

% \chapter{Inserting the index}
% After using the \verb"\makeindex" macro and loading the
% \texttt{makeidx} package that provides additional indexing
% commands, index entries can be created by issuing the \verb"\index"
% command. \index{dummy text|(}It is possible to create ranged index
% entries, which will encompass a span of text.\index{dummy text|)}
% To insert complex typographic material -- such as $\alpha$
% \index{alpha@$\alpha$} or \TeX{} \index{TeX@\TeX} --
% into the index, you need to specify a text string, which will
% determine how the entry will be sorted. It is also possible to
% create hierarchal entries. \index{vehicles!trucks}
% \index{vehicles!speed cars}

% After typesetting the document, it is necessary to generate the
% index by running
% \begin{center}%
%   \texttt{texindy -I latex -C utf8 -L }$\langle$\textit{locale}%
%   $\rangle$\texttt{ \jobname.idx}
% \end{center}
% from the command line, where $\langle$\textit{locale}$\rangle$
% corresponds to the main locale of your thesis -- such as
% \texttt{english}, and then typesetting the document again.

% The \texttt{texindy} command needs to be executed from within the
% directory, where the \LaTeX\ source file is located. In Windows,
% the command line can be opened in a directory by holding down the
% \textsf{Shift} key and by clicking the right mouse button while
% hovering the cursor over a directory. Select the \textsf{Open Command
% Window Here} option in the context menu that opens shortly
% afterwards.

% With online services -- such as Overleaf -- the commands are
% executed automatically, although the locale may be erroneously
% detected, or the \texttt{makeindex} tool (which is only able to
% sort entries that contain digits and letters of the English
% alphabet) may be used instead of \texttt{texindy}. In either case,
% the index will be ill-sorted.

  \makeatletter\thesis@blocks@clear\makeatother
  \phantomsection %% Print the index and insert it into the
  \addcontentsline{toc}{chapter}{\indexname} %% table of contents.
  \printindex

\appendix %% Start the appendices.
% \chapter{An appendix}
% Here you can insert the appendices of your thesis.

\end{document}
